                     1. Introduction to Artificial Intelligence (AI) and Machine Learning (ML)
 Artificial Intelligence (AI) refers to the simulation of human intelligence in machines. These machines are designed to think, learn, reason, and problem solve like humans. AI can be categorized into two types:
 Narrow AI: Designed for specific tasks (e.g., voice assistants like Siri).
General AI: A system that exhibits humanlike intelligence across a wide range of tasks (this is still a concept and not achieved yet).
Machine Learning (ML) is a subset of AI. It is a method of data analysis that automates analytical model building. ML allows computers to learn from data without being explicitly programmed.

                      2. Early History of AI
  
1940s1950s: 
Alan Turing proposed the idea that machines could simulate any conceivable act of mathematical deduction. His famous paper, "Computing Machinery and Intelligence" (1950), introduced the Turing Test as a criterion for intelligence.
The concept of a stored program computer—which could be reprogrammed for different tasks—was key to AI’s early development.

1956  Dartmouth Conference: 
AI as a field of study began here. The term Artificial Intelligence was coined by John McCarthy, and the conference proposed that “every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.”

                      3. The Growth of AI (1950s–1970s)
1950s1960s: 
The focus was on symbolic AI and rulebased systems.
Newell and Simon developed the Logic Theorist (1955), which proved mathematical theorems.
 McCarthy developed LISP, the first AI programming language, and Marvin Minsky worked on neural networks in early AI.
  
1970s: 
Symbolic AI struggled with realworld knowledge and flexibility. For example, programs like SHRDLU (Terry Winograd’s natural language understanding system) could only work in simple domains (like blocks world).
The First AI Winter: In the mid1970s, progress slowed due to limitations in computing power, overhyped expectations, and the complexity of realworld problems.
